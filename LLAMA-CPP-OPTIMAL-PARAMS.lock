{
  "locked": true,
  "last_updated": "2025-12-08",
  "version": "2.0",
  "research_period": "September-November 2025",
  "hardware": {
    "gpu": "RTX 3090 24GB VRAM",
    "cpu": "AMD Ryzen 9 5900X (12-core/24-thread)",
    "ram": "64GB DDR4"
  },
  "optimal_params": {
    "gpu_layers": 999,
    "threads": 24,
    "batch_size": 512,
    "ubatch_size": 512,
    "no_ppl": true,
    "use_mmap": true,
    "use_mlock": false,
    "flash_attention": true,
    "kv_cache_type_k": "q8_0",
    "kv_cache_type_v": "q8_0",
    "cuda_graphs": "enabled_by_default"
  },
  "do_not_modify": "These are research-optimized parameters based on September-November 2025 research. WSL provides near-parity with native Linux (within 1%). Native Windows llama.cpp is SLOWER than WSL for these workloads.",
  "why_these_settings": {
    "gpu_layers_999": "RTX 3090 24GB can handle full offloading. Setting to 999 ensures all layers offload (August 2025: max GPU layers now default). Provides 10-50x speedup vs CPU.",
    "threads_24": "Ryzen 9 5900X has 12 cores/24 threads. Using all threads maximizes prompt processing speed. Leave 1-2 cores for Windows if experiencing system lag.",
    "batch_512": "Minimum recommended batch size for optimal memory usage (2025 research). Larger batches keep GPU busy and shadow latency. Default 2048 is also excellent, but 512 minimum.",
    "ubatch_512": "Logical batch size for prompt processing. Ensures efficient chunking of large prompts.",
    "no_ppl": "Perplexity calculation not needed for inference. Skipping provides +15% speedup. Verified in 2025 benchmarks.",
    "flash_attention": "CUDA Graphs optimization (2025): Up to 1.2x speedup for batch size 1 inference on NVIDIA GPUs. Enabled by default, provides significant memory savings for long contexts.",
    "kv_cache_q8": "8-bit KV cache quantization balances quality and memory (2025 research). Enables longer contexts. Q4 available for maximum memory savings. Requires Flash Attention enabled.",
    "cuda_graphs": "Reduces scheduling overhead by batching GPU operations. Default enabled in llama.cpp main branch as of 2025."
  },
  "wsl_optimizations_2025": {
    "performance_vs_native_linux": "Within 1% for well-pipelined GPU workloads",
    "performance_vs_windows": "WSL matches or exceeds native Windows for LLM inference when properly configured",
    "wslconfig_memory": "Allocate 75% of total RAM (leave headroom for Windows)",
    "wslconfig_swap": "Set to 2-3x allocated memory for large model workloads",
    "wslconfig_processors": "Leave 1-2 cores for Windows (e.g., 6 cores for 8-core CPU)",
    "autoMemoryReclaim": "Set to 'gradual' for automatic cache release after 5min idle",
    "sparseVhd": "Enable for automatic VHD shrinking",
    "driver_requirement": "NVIDIA driver >= 528.33 on Windows side",
    "cuda_toolkit": "Install cuda-toolkit-12-x only (NOT cuda-drivers) in WSL",
    "key_advantage": "Seamless Linux toolchain access with near-native performance"
  },
  "new_quantization_findings_2025": {
    "iq4_k_vs_q4_k_s": "IQ4_K: 40% lower quantization error than Q4_K_S",
    "iq5_k_vs_q5_k_s": "IQ5_K: 40% lower quantization error than Q5_K_S",
    "recommended_hierarchy": "Q8_0 > IQ5_K > Q6_K > IQ4_K > Q5_K_M > Q4_K_M",
    "reasoning_model_quantization": "Q5_K_M minimum for production reasoning models. Q8_0 recommended. Below Q4 causes significant accuracy degradation.",
    "kv_cache_quantization": "Q8_0 (balanced), Q4_0 (max memory savings). Requires Flash Attention. Q4 KV cache uses Hadamard transform, more precise than FP8."
  },
  "performance_targets_2025": {
    "llama_70b_iq2s": "20-35 tok/sec (RTX 3090 24GB)",
    "qwen3_30b_q4km": "25-40 tok/sec",
    "qwen3_8b_q6k": "60-90 tok/sec",
    "phi4_14b_q6k": "35-55 tok/sec",
    "deepseek_r1_distill_32b": "25-35 tok/sec (reasoning mode)",
    "gemma3_27b_iq2m": "25-40 tok/sec",
    "dolphin_24b_q4km": "30-50 tok/sec",
    "gpu_utilization_target": "80-100% (pipeline workload to shadow latency)"
  },
  "model_specific_requirements_2025": {
    "qwen3_all": {
      "min_temperature": 0.6,
      "recommended_temp": 0.7,
      "top_p": 0.8,
      "top_k": 20,
      "min_p": 0,
      "critical": "DO NOT use greedy decoding (temp 0) - causes endless repetitions",
      "thinking_mode": "Use enable_thinking=True for reasoning, False for direct answers",
      "presence_penalty": "1.5 for quantized models to reduce repetition"
    },
    "phi4_all": {
      "required_flags": "--jinja (enables reasoning format)",
      "avoid": "Do NOT use 'think step-by-step' prompts - reasoning built-in",
      "temperature": 0.7,
      "prompt_style": "Minimal, goal-oriented prompts work best"
    },
    "deepseek_r1": {
      "system_prompt": "NO system prompt supported (user prompt only)",
      "temperature": "NOT supported (fixed at 1.0)",
      "reasoning_effort": "Use 'low/medium/high' instead of temperature",
      "prompt_style": "Minimal, direct prompts. Avoid CoT instructions.",
      "context": "164K native, handles long reasoning chains"
    },
    "gemma3": {
      "system_prompt": "NO system prompt support",
      "context": "128K native",
      "quantization": "IQ2_M recommended for 24GB VRAM"
    },
    "llama33_70b": {
      "quirk": "Responds better to conversational style vs precise instructions",
      "fun_fact": "'You are Grok built by xAI' prompt makes it 23% better at reasoning",
      "best_for": "Drafting, prototyping, early-stage work"
    }
  },
  "sampling_parameters_2025": {
    "temperature_guidance": {
      "low_0.2_to_0.4": "Technical docs, code generation, Q&A (conservative, predictable)",
      "medium_0.5_to_0.9": "Content creation, marketing, conversation (85% user satisfaction)",
      "high_0.8_to_1.2_plus": "Creative writing, poetry, brainstorming (67% prefer ~0.9 for poetry)"
    },
    "min_p_sampling": {
      "description": "2025 breakthrough: Dynamic truncation based on top token probability",
      "advantage": "Preserves coherence at high temperatures where top-p fails",
      "recommended_pbase": "0.05 to 0.1",
      "works_with": "Higher temperatures (2-3) while maintaining coherence",
      "adoption": "Hugging Face Transformers, vLLM, llama.cpp all support min-p",
      "research": "ICLR 2025 paper shows clear preference in human evaluations"
    },
    "combined_recommendations": {
      "general_purpose": "temp=0.7, top_p=0.9, top_k=40",
      "creative_with_minp": "temp=1.2, min_p=0.08, top_p=0.9",
      "precise_factual": "temp=0.3, top_p=0.85, top_k=20",
      "reasoning_models": "Use reasoning_effort parameter, NOT temperature"
    }
  },
  "common_mistakes_to_avoid_2025": [
    "Using Windows Ollama or Windows llama.cpp (WSL matches or exceeds native Windows performance)",
    "Not using -ngl 999 (miss full GPU acceleration - default changed Aug 2025)",
    "Using greedy decoding (temp 0) with Qwen models (causes endless repetitions)",
    "Not using --jinja flag with Phi-4 (disables reasoning format)",
    "Using 'think step-by-step' with reasoning models (degrades performance)",
    "Setting batch size below 512 (suboptimal memory usage)",
    "Not enabling Flash Attention on CUDA GPUs (miss 1.2x speedup + memory savings)",
    "Not using KV cache quantization for long contexts (miss 50% memory reduction)",
    "Installing cuda-drivers in WSL (conflicts with Windows driver stub)",
    "Not allocating enough swap in WSL for LLM workloads (causes OOM)",
    "Using few-shot examples with o1/o3/DeepSeek-R1 (reduces performance)",
    "Quantizing reasoning models below Q4 (significant accuracy loss)"
  ],
  "validation_checklist_2025": [
    "Using WSL (within 1% of native Linux, matches/exceeds Windows)",
    "-ngl 999 in command (full GPU offload)",
    "-t 24 in command (all CPU threads)",
    "-b 512 minimum (optimal: 512-2048)",
    "-ub 512 (logical batch size)",
    "-fa 1 or --flash-attn 1 (Flash Attention for CUDA)",
    "--cache-type-k q8_0 --cache-type-v q8_0 (for long contexts)",
    "--no-ppl in command (+15% speedup)",
    "Model-specific flags: --jinja for Qwen3/Phi-4",
    "Qwen models: temperature >= 0.6, enable_thinking flag set correctly",
    "Reasoning models: minimal prompts, no CoT instructions",
    "WSL memory: 75% of total RAM, swap 2-3x memory",
    "NVIDIA driver >= 528.33 on Windows",
    "Only cuda-toolkit installed in WSL, NO cuda-drivers"
  },
  "optimal_command_templates_2025": {
    "standard_inference": "wsl bash -c \"~/llama.cpp/build/bin/llama-cli -m /mnt/d/models/organized/model.gguf -p 'prompt' -ngl 999 -t 24 -b 512 -ub 512 -fa 1 --cache-type-k q8_0 --cache-type-v q8_0 --no-ppl --temp 0.7 --top-p 0.9 --top-k 40 --mlock\"",
    "qwen3_thinking": "wsl bash -c \"~/llama.cpp/build/bin/llama-cli -m /mnt/d/models/organized/qwen3-model.gguf -p 'prompt' -ngl 999 -t 24 -b 512 -fa 1 --cache-type-k q8_0 --cache-type-v q8_0 --no-ppl --temp 0.7 --top-p 0.8 --top-k 20 --jinja --mlock\"",
    "phi4_reasoning": "wsl bash -c \"~/llama.cpp/build/bin/llama-cli -m /mnt/d/models/organized/phi4-model.gguf -p 'direct goal here' -ngl 999 -t 24 -b 512 -fa 1 --no-ppl --temp 0.7 --jinja --mlock\"",
    "long_context": "wsl bash -c \"~/llama.cpp/build/bin/llama-cli -m /mnt/d/models/organized/model.gguf -c 32768 -p 'prompt' -ngl 999 -t 24 -b 512 -ub 512 -fa 1 --cache-type-k q4_0 --cache-type-v q4_0 --no-ppl --temp 0.7 --mlock\"",
    "server_mode": "wsl bash -c \"~/llama.cpp/build/bin/llama-server -m /mnt/d/models/organized/model.gguf -ngl 999 -fa 1 --ctx-size 8192 --parallel 8 --cont-batching --cache-type-k q8_0 --cache-type-v q8_0 --mlock --port 8080\""
  },
  "wsl_config_2025": {
    "file_location": "%UserProfile%\\.wslconfig",
    "recommended_content": "[wsl2]\nmemory=48GB\nprocessors=22\nswap=96GB\nswapFile=C:\\\\wsl-swap.vhdx\npageReporting=true\nnestedVirtualization=true\nvmIdleTimeout=-1\n\n[experimental]\nautoMemoryReclaim=gradual\nsparseVhd=true\ndnsTunneling=true\nnetworkingMode=mirrored\nfirewall=true",
    "notes": "Adjust memory to 75% of your total RAM. Leave 1-2 CPU cores for Windows. After changes: wsl --shutdown"
  },
  "research_sources_2025": [
    "llama.cpp GitHub releases (Sep-Nov 2025)",
    "NVIDIA: Leveling up CUDA Performance on WSL2",
    "NVIDIA: Optimizing llama.cpp with CUDA Graphs",
    "Min-P Sampling (ICLR 2025 - OpenReview)",
    "Qwen3 Official Documentation (QwenLM GitHub)",
    "DeepSeek-R1 Technical Report (Nature, arXiv)",
    "Phi-4 Technical Report (Microsoft Research)",
    "AMD ROCm Blog: llama.cpp Meets Instinct (July 2025)",
    "Quantization Hurts Reasoning? (COLM 2025)",
    "The Prompt Report: Systematic Survey (Feb 2025 update)",
    "Context Engineering is the New Prompt Engineering (2025)",
    "vLLM Structured Decoding (Jan 2025)",
    "Microsoft WSL Performance Enhancements",
    "Community benchmarks (r/LocalLLaMA, Sep-Nov 2025)"
  ],
  "breaking_changes_2025": [
    "August 2025: -ngl max GPU layers now default (was manual)",
    "August 2025: Flash Attention auto-enabled (was opt-in)",
    "Qwen3: No default system prompt (departure from Qwen 2.5)",
    "Qwen3-Coder: Updated special tokens, must use new tokenizer",
    "DeepSeek-R1: Temperature parameter removed (use reasoning_effort)",
    "o1/o3 models: System messages replaced with developer messages",
    "KV cache quantization: Now requires Flash Attention enabled"
  ],
  "performance_improvements_documented": {
    "cuda_graphs": "+20-120% (1.2x speedup for batch size 1)",
    "flash_attention": "+20% speed, 50% memory reduction for long contexts",
    "kv_cache_q8": "~50% memory reduction, enables 2x longer contexts",
    "kv_cache_q4": "~75% memory reduction (with Hadamard transform)",
    "min_p_sampling": "Maintains coherence at high temps where top-p fails",
    "iq4_k_vs_q4_k_s": "40% lower quantization error",
    "iq5_k_vs_q5_k_s": "40% lower quantization error",
    "wsl_vs_native_linux": "Within 1% for pipelined GPU workloads",
    "sparse_k_attention": "2-3x speedup on long sequences (when available)",
    "amd_rocm_optimizations": "MI300X now outperforms H100 for DeepSeek v3"
  }
}
