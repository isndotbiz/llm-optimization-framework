CLAUDE MODEL MANAGEMENT WARP PROMPT
=====================================

Copy and paste this entire text into Warp CLI to run local analysis without AI API costs.

---

## Your LLM Model Status

You now have an optimized, dual-GPU model arsenal:

### RTX 3090 (24GB) - 77GB Total
UNCENSORED STACK:
- Llama 3.3 70B Abliterated (IQ2_S, 21GB) - Best 2025 uncensored
- Dolphin-Mistral-24B-Venice (Q4_K_M, 14GB) - 2.2% refusal rate
- MythoMax-L2-13B (Q6_K, 10GB) - Creative writing
- Dolphin3.0-Llama3.1-8B (Q6_K, 6.2GB) - Fast 8B uncensored
- Wizard-Vicuna-13B (Q4_0, 6.9GB) - Classic uncensored

CENSORED/SMART STACK:
- Qwen2.5-Coder-32B (Q4_K_M, 19GB) - Best 2025 coding
- Phi-4 14B (Q6_K, 12GB) - Best reasoning
- DeepSeek-R1-Distill-Qwen-14B (Q5_K_M, 9.8GB) - Chain-of-thought

### RTX 4060 Ti (16GB) - 32GB Total (Server)
- Qwen 2.5 14B Instruct (Q4_K_M, 9GB) - Daily driver
- Qwen 2.5 Coder 7B (Q5_K_M, 5.1GB) - Coding specialist
- Llama 3.1 8B (Q6_K, 6.2GB) - General purpose
- Qwen 2.5 14B Uncensored (Q4_K_M, 8.4GB) - Unrestricted
- Dolphin3.0-Llama3.1-8B (Q6_K, 6.2GB) - Fast uncensored

---

## Quick Commands

List all models:
.\run-model.ps1 -ListModels

Test Llama 3.3 70B uncensored:
.\run-model.ps1 -ModelName "llama-3.3" -Prompt "Your prompt here" -MaxTokens 512

Interactive chat with Dolphin Mistral:
.\run-model.ps1 -ModelName "dolphin-mistral" -Interactive

Code generation with Qwen Coder:
.\run-model.ps1 -ModelName "qwen-coder" -Prompt "Write Python code for..." -MaxTokens 1024

Reasoning with Phi-4:
.\run-model.ps1 -ModelName "phi-4" -Prompt "Complex problem here..." -Temperature 0.3

---

## Model Optimization Rationale

### Why These Models?

VRAM EFFICIENCY:
- Llama 70B at IQ2_S (21GB) instead of larger quants = fits 24GB with context room
- Qwen Coder 32B at Q4_K_M (19GB) instead of Q4_K_L = saves 1.6GB for KV cache
- MythoMax 13B at Q6_K vs older Q5_K_M = better quality for creative tasks

QUALITY TIERS:
- IQ2_S: Best for 70B on 24GB (10% better than IQ2_XS)
- Q4_K_M: Community "sweet spot" for 32B models
- Q6_K: Near-original quality for 13B models
- Q8_0: Rarely used (only when VRAM allows)

DELETED MODELS (Why?):
- gemma-2-27b (16GB) → Redundant, less capable than Qwen
- neural-chat-7b (4.1GB) → Redundant, have Dolphin 3.0
- openchat-3.5 (4.1GB) → Redundant
- openhermes-2.5-mistral-7b → Redundant with Dolphin 3.0

---

## Performance Expectations

24GB RTX 3090:
- 70B uncensored: 5-15 tokens/sec (CPU offload) or 20-40 tok/sec (full VRAM)
- 32B coded: 25-35 tokens/sec
- 13-14B models: 40-60 tokens/sec
- 8B models: 60-100 tokens/sec

16GB RTX 4060 Ti:
- 14B models: 20-30 tokens/sec
- 7-8B models: 40-60 tokens/sec

Context windows at full VRAM:
- 70B: 6-8K tokens
- 32B: 12-16K tokens
- 14B: 24-32K tokens
- 8B: 48-64K tokens

---

## For Future: Advanced Usage

GPU OFFLOADING (if needed for 70B):
.\run-model.ps1 -ModelName "llama-3.3" -Prompt "text" -MaxTokens 128 -ExtraParams "-ngl 20"

BATCH PROCESSING (run same prompt on multiple models):
foreach ($model in @("llama-3.3", "dolphin-mistral", "phi-4")) {
    .\run-model.ps1 -ModelName $model -Prompt "your question" -MaxTokens 256
}

SYSTEM PROMPT CONTROL:
Create wrapper script that passes custom system prompts to models

---

## File Locations

Model Registry: D:\models\model-registry.json
Organized Models: D:\models\organized\
RTX 3090 Models: D:\models\organized\
RTX 4060 Models: D:\models\rtx4060ti-16gb\

Scripts:
- run-model.ps1 → Run any model with custom parameters
- organize-models.ps1 → Rescans and updates registry

---

## Key Takeaways

✅ OPTIMIZED: Every model is in the best quantization for your hardware
✅ EFFICIENT: Freed ~97GB by deleting redundant models
✅ BALANCED: Mix of uncensored (research) and censored (smart) models
✅ FUTURE-READY: Latest 2025 models (Llama 3.3, Qwen 2.5, Phi-4, etc.)
✅ DUAL-GPU: Both main PC and server have optimal configurations

Your setup is now production-ready. No more API costs for model testing!
