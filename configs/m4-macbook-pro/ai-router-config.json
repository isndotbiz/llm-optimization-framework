{
  "machine": {
    "id": "m4-macbook-pro",
    "name": "M4 MacBook Pro",
    "specs": "12-core CPU, 16-core GPU, 24GB unified memory",
    "platform": "Darwin"
  },
  "models": {
    "primary": "qwen25-coder-7b-mlx",
    "alternatives": ["mistral-7b-mlx", "phi-3-mini-mlx"],
    "enabled": true
  },
  "performance": {
    "max_tokens": 2048,
    "batch_size": 1,
    "temperature": 0.7,
    "top_p": 0.95
  },
  "inference": {
    "backend": "mlx",
    "device": "gpu",
    "dtype": "float16",
    "attention": "flash_attention"
  },
  "venv": {
    "path": "configs/m4-macbook-pro/venv",
    "python_version": "3.11"
  },
  "model_paths": {
    "base": "configs/m4-macbook-pro/models",
    "cache": "configs/m4-macbook-pro/cache"
  },
  "logging": {
    "level": "INFO",
    "file": "configs/m4-macbook-pro/logs/ai-router.log"
  },
  "notes": [
    "MLX-optimized for Apple Silicon (M4 Pro)",
    "12-core CPU with integrated 16-core GPU",
    "Unified 24GB memory (CPU and GPU share)",
    "Conservative settings to prevent thermal throttling during long runs",
    "Excellent for interactive development and code review",
    "Portable - take anywhere",
    "Can handle 7B-13B models smoothly"
  ]
}
