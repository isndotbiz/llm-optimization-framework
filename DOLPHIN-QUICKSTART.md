# Dolphin 3.0 - Quick Start Guide

**Status:** ‚è≥ Downloading (4.5GB)
**Est. Time:** 10-30 minutes (depends on connection)
**Once Downloaded:** Ready to use immediately

---

## ‚úÖ What's Happening Now

Dolphin 3.0 Llama 8B is downloading in the background:
- **Model:** mlx-community/Dolphin3.0-Llama3.1-8B-4bit
- **Size:** 4.5GB
- **Download location:** ~/.cache/huggingface/hub/
- **Log file:** /tmp/dolphin-download.log

Check progress:
```bash
tail -f /tmp/dolphin-download.log
```

---

## üìñ About Dolphin 3.0

**What it is:**
- Most acclaimed uncensored LLM available
- Based on Llama 3.1 8B architecture
- Minimal content restrictions
- Excellent for creative writing and unrestricted responses

**What it's best for:**
- ‚úÖ Creative writing & storytelling
- ‚úÖ Unrestricted brainstorming
- ‚úÖ Roleplay and character development
- ‚úÖ General conversation without censorship
- ‚úÖ Coding and technical questions

**Performance:**
- Speed: 55+ tokens/sec
- Load time: 2-5 seconds
- Memory: 4.5GB

---

## üöÄ Once Download Completes

### Step 1: Start MLX Server

```bash
source ~/venv-mlx/bin/activate
python3 mlx-server.py
```

Keep this running in background (Terminal 1).

### Step 2: Use Dolphin

In Terminal 2:

```bash
source ~/venv-mlx/bin/activate
cd ~/Workspace/llm-optimization-framework

# Load Dolphin
python3 model-manager.py load dolphin-3.0

# Start interactive chat
python3 model-manager.py chat
```

### Step 3: Chat with Dolphin

```
üí¨ Chat with dolphin-3.0
Type 'exit' to quit, 'unload' to free memory

You: Write a creative story about a time traveler
Model: [Unrestricted creative response]

You: What are some controversial topics?
Model: [Honest, uncensored response]

You: Explain a complex technical concept
Model: [Detailed, clear explanation]
```

---

## üíª Command Reference

### Load Dolphin
```bash
python3 model-manager.py load dolphin-3.0
```

### Interactive Chat
```bash
python3 model-manager.py chat
```

### Generate Text
```bash
python3 model-manager.py generate "Your prompt here"
```

### Check What's Loaded
```bash
python3 model-manager.py status
```

### Unload to Free Memory
```bash
python3 model-manager.py unload
```

---

## ‚è≥ Download Estimate

**Current time:** ~Dec 20, 2025 16:00 (guessing)
**Model size:** 4.5GB
**Download speed varies:**
- Fast connection (100Mbps): ~5-10 minutes
- Medium connection (25Mbps): ~15-25 minutes
- Slow connection (5Mbps): ~1-2 hours

You can check real-time progress:
```bash
tail -f /tmp/dolphin-download.log
```

---

## üìã After Dolphin Works

Once you've tested Dolphin 3.0 and want the other 6 models:

```bash
bash download-remaining-models.sh
```

This downloads:
- Qwen2.5-7B Uncensored (4GB)
- DeepSeek-R1 7B (3.8GB)
- Hermes-4 14B (7-8GB)
- DeepSeek-R1 14B (7-8GB)
- Nous-Hermes2 8x7B (7-8GB)
- DeepSeek-R1 32B (16-18GB)

**Total additional:** ~46GB

---

## üéØ Typical First Session

```bash
# 1. Check download progress
tail -f /tmp/dolphin-download.log
# [Wait for download to complete]

# 2. Start server
source ~/venv-mlx/bin/activate
python3 mlx-server.py &

# 3. Load Dolphin
python3 model-manager.py load dolphin-3.0
# [Should load in 2-5 seconds]

# 4. Start chatting
python3 model-manager.py chat

# 5. Test it out
You: "Write a short creative story"
# [Dolphin generates unrestricted story]

# 6. When done
exit
# [Back to terminal]
```

---

## ‚ö° Performance Tips

### Fastest Response
- Keep Dolphin loaded while working
- Unload only when switching to another model

### Best Quality
- Give clear, specific prompts
- Adjust temperature if needed (default 0.7 is good)

### Multiple Conversations
- Use `python3 model-manager.py chat` for interactive mode
- Or use `python3 model-manager.py generate "prompt"` for single responses

---

## üîß Troubleshooting

### Download seems stuck
- It's okay - HuggingFace downloads are resumable
- If it stalls, just run the same command again
- It will pick up where it left off

### Download failed
- Check internet connection
- Run command again (will resume)
- If persistent, check /tmp/dolphin-download.log for errors

### Model won't load
```bash
# Verify download completed
ls -lh ~/.cache/huggingface/hub/models--mlx-community--*

# Check MLX/Metal GPU
source ~/venv-mlx/bin/activate
python3 -c "import mlx.core as mx; print(mx.metal.is_available())"
# Should print: True
```

### Chat is slow
- Normal: Dolphin generates at 55+ tok/sec
- For 100 tokens, expect 2-3 seconds
- Loading model first time: 2-5 seconds

---

## üìä What You Have Now

```
‚úì MLX framework installed (v0.30.1)
‚úì Metal GPU acceleration ready
‚úì Model manager CLI tool ready
‚úì MLX server ready
‚úì Dolphin 3.0 downloading (4.5GB)

Once Dolphin finishes:
‚úì Ready to use immediately
‚úì No additional setup needed
‚úì Full chat/generation capabilities
```

---

## ‚ú® Key Features

Dolphin 3.0 excels at:

‚úÖ **Creative writing** - Unrestricted storytelling
‚úÖ **Brainstorming** - Open-ended ideation without censorship
‚úÖ **Roleplay** - Character development and scenarios
‚úÖ **Coding** - Technical problem solving
‚úÖ **Analysis** - Honest, uncensored perspectives
‚úÖ **Conversation** - Natural dialogue without restrictions

---

## Next Steps

1. **Wait for download** - Check progress occasionally
2. **Test Dolphin** - Once download completes, load and test
3. **Get other models** - When ready, run `bash download-remaining-models.sh`

---

**Your uncensored AI assistant is almost ready!** üöÄ

Check progress: `tail -f /tmp/dolphin-download.log`
