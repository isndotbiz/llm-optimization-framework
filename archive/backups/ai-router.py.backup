#!/usr/bin/env python3
"""
AI Router - Intelligent Model Selection and Execution CLI
Optimized for RTX 3090 (WSL) and MacBook M4 Pro
Based on 2025 Research Findings (Sep-Nov 2025)
"""

import os
import sys
import subprocess
import json
import platform
from pathlib import Path
from dataclasses import dataclass
from datetime import datetime
from typing import Optional, Dict, Any
import time
import re
from response_processor import ResponseProcessor
from model_selector import ModelSelector
from context_manager import ContextManager
from template_manager import TemplateManager

def is_wsl():
    """Detect if running in WSL (Windows Subsystem for Linux)"""
    try:
        with open('/proc/version', 'r') as f:
            return 'microsoft' in f.read().lower()
    except:
        return False

# Color codes for terminal output
class Colors:
    RESET = '\033[0m'
    BOLD = '\033[1m'
    DIM = '\033[2m'

    # Foreground colors
    BLACK = '\033[30m'
    RED = '\033[31m'
    GREEN = '\033[32m'
    YELLOW = '\033[33m'
    BLUE = '\033[34m'
    MAGENTA = '\033[35m'
    CYAN = '\033[36m'
    WHITE = '\033[37m'

    # Bright foreground colors
    BRIGHT_BLACK = '\033[90m'
    BRIGHT_RED = '\033[91m'
    BRIGHT_GREEN = '\033[92m'
    BRIGHT_YELLOW = '\033[93m'
    BRIGHT_BLUE = '\033[94m'
    BRIGHT_MAGENTA = '\033[95m'
    BRIGHT_CYAN = '\033[96m'
    BRIGHT_WHITE = '\033[97m'

    # Background colors
    BG_BLACK = '\033[40m'
    BG_RED = '\033[41m'
    BG_GREEN = '\033[42m'
    BG_YELLOW = '\033[43m'
    BG_BLUE = '\033[44m'
    BG_MAGENTA = '\033[45m'
    BG_CYAN = '\033[46m'
    BG_WHITE = '\033[47m'


@dataclass
class ModelResponse:
    """Captures a complete model response with metadata"""
    text: str
    model_id: str
    model_name: str
    tokens_input: Optional[int] = None
    tokens_output: Optional[int] = None
    duration_seconds: Optional[float] = None
    timestamp: datetime = None
    metadata: Optional[Dict[str, Any]] = None

    def __post_init__(self):
        if self.timestamp is None:
            self.timestamp = datetime.now()


class ModelDatabase:
    """Comprehensive model database with 2025 research-optimized settings"""

    # RTX 3090 Models (WSL)
    RTX3090_MODELS = {
        "qwen3-coder-30b": {
            "name": "Qwen3 Coder 30B Q4_K_M",
            "path": "/mnt/d/models/organized/Qwen3-Coder-30B-A3B-Instruct-Q4_K_M.gguf",
            "size": "18GB",
            "speed": "25-35 tok/sec",
            "use_case": "Advanced coding, code review, architecture design",
            "temperature": 0.7,
            "top_p": 0.8,
            "top_k": 20,
            "context": 32768,
            "special_flags": ["--jinja"],
            "system_prompt": "system-prompt-qwen3-coder-30b.txt",
            "notes": "CRITICAL: Never use temp 0 (causes endless loops). Use enable_thinking for reasoning.",
            "framework": "llama.cpp"
        },
        "qwen25-coder-32b": {
            "name": "Qwen2.5 Coder 32B Q4_K_M",
            "path": "/mnt/d/models/organized/Qwen2.5-Coder-32B-Instruct-Q4_K_M.gguf",
            "size": "19GB",
            "speed": "25-35 tok/sec",
            "use_case": "Coding, debugging, technical documentation",
            "temperature": 0.7,
            "top_p": 0.9,
            "top_k": 40,
            "context": 32768,
            "special_flags": [],
            "system_prompt": "system-prompt-qwen25-coder-32b.txt",
            "notes": "Excellent for code generation. Use temp >= 0.6.",
            "framework": "llama.cpp"
        },
        "phi4-14b": {
            "name": "Phi-4 Reasoning Plus 14B Q6_K",
            "path": "/mnt/d/models/organized/microsoft_Phi-4-reasoning-plus-Q6_K.gguf",
            "size": "12GB",
            "speed": "35-55 tok/sec",
            "use_case": "Math, reasoning, STEM, logical analysis",
            "temperature": 0.7,
            "top_p": 0.9,
            "top_k": 40,
            "context": 16384,
            "special_flags": ["--jinja"],
            "system_prompt": "system-prompt-phi4-14b.txt",
            "notes": "CRITICAL: Requires --jinja flag. DO NOT use 'think step-by-step' prompts.",
            "framework": "llama.cpp"
        },
        "gemma3-27b": {
            "name": "Gemma 3 27B Q2_K (Abliterated)",
            "path": "/mnt/d/models/organized/mlabonne_gemma-3-27b-it-abliterated-Q2_K.gguf",
            "size": "10GB",
            "speed": "25-40 tok/sec",
            "use_case": "Uncensored chat, creative writing, research",
            "temperature": 0.9,
            "top_p": 0.9,
            "top_k": 40,
            "context": 128000,
            "special_flags": [],
            "system_prompt": None,
            "notes": "NO system prompt support. 128K context. Uncensored/abliterated variant.",
            "framework": "llama.cpp"
        },
        "ministral-3-14b": {
            "name": "Ministral-3 14B Reasoning Q5_K_M",
            "path": "/mnt/d/models/organized/Ministral-3-14B-Reasoning-2512-Q5_K_M.gguf",
            "size": "9GB",
            "speed": "35-50 tok/sec",
            "use_case": "Complex reasoning, problem solving, analysis",
            "temperature": 0.7,
            "top_p": 0.9,
            "top_k": 40,
            "context": 262144,
            "special_flags": [],
            "system_prompt": "system-prompt-ministral-3-14b.txt",
            "notes": "256K context window. Excellent for long-context reasoning.",
            "framework": "llama.cpp"
        },
        "deepseek-r1-14b": {
            "name": "DeepSeek R1 Distill Qwen 14B Q5_K_M",
            "path": "/mnt/d/models/organized/DeepSeek-R1-Distill-Qwen-14B-Q5_K_M.gguf",
            "size": "10GB",
            "speed": "30-50 tok/sec",
            "use_case": "Advanced reasoning, research, complex analysis",
            "temperature": 0.7,
            "top_p": 0.9,
            "top_k": 40,
            "context": 32768,
            "special_flags": [],
            "system_prompt": "system-prompt-deepseek-r1.txt",
            "notes": "DeepSeek R1 distilled to Qwen. Excellent reasoning capabilities.",
            "framework": "llama.cpp"
        },
        "llama33-70b": {
            "name": "Llama 3.3 70B Instruct IQ2_S (Abliterated)",
            "path": "/mnt/d/models/organized/Llama-3.3-70B-Instruct-abliterated-IQ2_S.gguf",
            "size": "21GB",
            "speed": "15-25 tok/sec",
            "use_case": "Large-scale reasoning, research, uncensored tasks",
            "temperature": 0.7,
            "top_p": 0.9,
            "top_k": 40,
            "context": 131072,
            "special_flags": [],
            "system_prompt": "system-prompt-llama33-70b.txt",
            "notes": "Largest model available. Excellent for complex tasks. Uncensored.",
            "framework": "llama.cpp"
        },
        "dolphin-llama31-8b": {
            "name": "Dolphin 3.0 Llama 3.1 8B Q6_K",
            "path": "/mnt/d/models/organized/Dolphin3.0-Llama3.1-8B-Q6_K.gguf",
            "size": "6GB",
            "speed": "45-65 tok/sec",
            "use_case": "Fast general tasks, uncensored chat, quick assistance",
            "temperature": 0.7,
            "top_p": 0.9,
            "top_k": 40,
            "context": 32768,
            "special_flags": [],
            "system_prompt": "system-prompt-dolphin-8b.txt",
            "notes": "Fastest model. Uncensored variant of Llama 3.1 8B.",
            "framework": "llama.cpp"
        },
        "dolphin-mistral-24b": {
            "name": "Dolphin Mistral 24B Venice Q4_K_M",
            "path": "/mnt/d/models/organized/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q4_K_M.gguf",
            "size": "14GB",
            "speed": "25-40 tok/sec",
            "use_case": "Uncensored chat, creative tasks, roleplay",
            "temperature": 0.8,
            "top_p": 0.9,
            "top_k": 40,
            "context": 32768,
            "special_flags": [],
            "system_prompt": None,
            "notes": "Venice Edition: Completely uncensored. No system prompt support.",
            "framework": "llama.cpp"
        },
        "wizard-vicuna-13b": {
            "name": "Wizard Vicuna 13B Uncensored Q4_0",
            "path": "/mnt/d/models/organized/Wizard-Vicuna-13B-Uncensored-Q4_0.gguf",
            "size": "7GB",
            "speed": "35-50 tok/sec",
            "use_case": "General uncensored chat, creative writing",
            "temperature": 0.8,
            "top_p": 0.9,
            "top_k": 40,
            "context": 8192,
            "special_flags": [],
            "system_prompt": "system-prompt-wizard-vicuna.txt",
            "notes": "Classic uncensored model. Smaller context window.",
            "framework": "llama.cpp"
        }
    }

    # MacBook M4 Pro Models (MLX)
    M4_MODELS = {
        "qwen25-14b-mlx": {
            "name": "Qwen2.5 14B Instruct Q5_K_M (MLX)",
            "path": "~/models/qwen25-14b",
            "size": "11GB",
            "speed": "50-70 tok/sec",
            "use_case": "General purpose, research, chat",
            "temperature": 0.7,
            "top_p": 0.9,
            "top_k": 40,
            "context": 32768,
            "special_flags": [],
            "system_prompt": "system-prompt-qwen25-14b.txt",
            "notes": "Best daily driver for M4. Use MLX for 2-3x speedup vs llama.cpp.",
            "framework": "mlx"
        },
        "qwen25-coder-14b-mlx": {
            "name": "Qwen2.5 Coder 14B Q4_K_M (MLX)",
            "path": "~/models/qwen25-coder-14b",
            "size": "8GB",
            "speed": "50-75 tok/sec",
            "use_case": "Coding, debugging, technical tasks",
            "temperature": 0.7,
            "top_p": 0.9,
            "top_k": 40,
            "context": 32768,
            "special_flags": [],
            "system_prompt": "system-prompt-qwen25-coder-14b.txt",
            "notes": "Best coding model for M4.",
            "framework": "mlx"
        },
        "phi4-14b-mlx": {
            "name": "Phi-4 14B Q6_K (MLX)",
            "path": "~/models/phi4-14b",
            "size": "12GB",
            "speed": "60-75 tok/sec",
            "use_case": "Math, reasoning, STEM tasks",
            "temperature": 0.7,
            "top_p": 0.9,
            "top_k": 40,
            "context": 16384,
            "special_flags": [],
            "system_prompt": "system-prompt-phi4-14b.txt",
            "notes": "Excellent for reasoning on M4.",
            "framework": "mlx"
        },
        "gemma3-9b-mlx": {
            "name": "Gemma-3 9B Q6_K (MLX)",
            "path": "~/models/gemma3-9b",
            "size": "8GB",
            "speed": "85-110 tok/sec",
            "use_case": "Fast responses, chat, general queries",
            "temperature": 0.9,
            "top_p": 0.9,
            "top_k": 40,
            "context": 128000,
            "special_flags": [],
            "system_prompt": None,
            "notes": "Speed champion on M4. NO system prompt support.",
            "framework": "mlx"
        }
    }

    @classmethod
    def get_platform_models(cls):
        """Return models appropriate for current platform"""
        system = platform.system()
        if system == "Darwin":  # macOS
            return cls.M4_MODELS
        else:  # Windows/WSL - RTX 3090
            return cls.RTX3090_MODELS

    @classmethod
    def detect_use_case(cls, prompt_text):
        """Intelligently detect use case from prompt"""
        prompt_lower = prompt_text.lower()

        # Coding keywords
        coding_keywords = ['code', 'function', 'class', 'programming', 'debug', 'error',
                          'python', 'javascript', 'java', 'c++', 'rust', 'implement',
                          'refactor', 'algorithm', 'api', 'script', 'bug']

        # Math/reasoning keywords
        reasoning_keywords = ['calculate', 'prove', 'theorem', 'math', 'equation',
                             'logic', 'reasoning', 'solve', 'problem', 'analyze',
                             'deduce', 'infer', 'probability', 'statistics']

        # Creative keywords
        creative_keywords = ['story', 'poem', 'creative', 'write', 'fiction',
                            'narrative', 'character', 'plot', 'imagine']

        # Research keywords
        research_keywords = ['research', 'analyze', 'explain', 'summary', 'what is',
                            'how does', 'why', 'compare', 'contrast']

        if any(kw in prompt_lower for kw in coding_keywords):
            return "coding"
        elif any(kw in prompt_lower for kw in reasoning_keywords):
            return "reasoning"
        elif any(kw in prompt_lower for kw in creative_keywords):
            return "creative"
        elif any(kw in prompt_lower for kw in research_keywords):
            return "research"
        else:
            return "general"

    @classmethod
    def recommend_model(cls, use_case, platform_models):
        """Recommend best model for use case"""
        if not platform_models:
            raise ValueError("No models available for this platform")

        recommendations = {
            "coding": ["qwen3-coder-30b", "qwen25-coder-32b", "qwen25-coder-14b-mlx"],
            "reasoning": ["phi4-14b", "ministral-3-14b", "phi4-14b-mlx"],
            "creative": ["gemma3-27b", "gemma3-9b-mlx"],
            "research": ["qwen3-14b", "qwen25-14b", "qwen25-14b-mlx"],
            "general": ["qwen25-14b", "qwen3-14b", "qwen25-14b-mlx"]
        }

        for model_id in recommendations.get(use_case, []):
            if model_id in platform_models:
                return model_id, platform_models[model_id]

        # Fallback to first available model
        return list(platform_models.items())[0]


class AIRouter:
    """Main AI Router application"""

    def __init__(self):
        self.platform = platform.system()
        self.models = ModelDatabase.get_platform_models()

        # Detect correct models directory based on platform
        if self.platform == "Windows":
            self.models_dir = Path("D:/models")
        elif is_wsl():
            self.models_dir = Path("/mnt/d/models")
        else:  # macOS or Linux
            self.models_dir = Path.home() / "models"

        self.system_prompts_dir = self.models_dir

        # Initialize response processor
        self.output_dir = self.models_dir / "outputs"
        self.response_processor = ResponseProcessor(self.output_dir)

        # Store last response for post-processing
        self.last_response = None
        self.last_model_name = None

        # Initialize model selector with preference learning
        self.model_selector = ModelSelector(
            self.models_dir / ".ai-router-preferences.json"
        )

        # Initialize context manager
        self.context_manager = ContextManager()

        # Initialize template manager
        templates_dir = self.models_dir / "prompt-templates"
        self.template_manager = TemplateManager(templates_dir)

        # Bypass permissions configuration
        self.bypass_mode = False
        self.config_file = self.models_dir / ".ai-router-config.json"
        self._load_config()

    def _load_config(self):
        """Load configuration from JSON file"""
        try:
            if self.config_file.exists():
                with open(self.config_file, 'r', encoding='utf-8') as f:
                    config = json.load(f)
                    self.bypass_mode = config.get('bypass_mode', False)
        except Exception as e:
            print(f"{Colors.BRIGHT_YELLOW}Warning: Could not load config: {e}{Colors.RESET}")
            self.bypass_mode = False

    def _save_config(self):
        """Save configuration to JSON file"""
        try:
            config = {
                'bypass_mode': self.bypass_mode,
                'version': '1.0'
            }
            with open(self.config_file, 'w', encoding='utf-8') as f:
                json.dump(config, f, indent=2)
        except Exception as e:
            print(f"{Colors.BRIGHT_RED}Error saving config: {e}{Colors.RESET}")

    def toggle_bypass_mode(self):
        """Toggle bypass/auto-yes mode on or off"""
        self.bypass_mode = not self.bypass_mode
        self._save_config()

        status = f"{Colors.BRIGHT_GREEN}ENABLED{Colors.RESET}" if self.bypass_mode else f"{Colors.BRIGHT_RED}DISABLED{Colors.RESET}"
        print(f"\n{Colors.BRIGHT_CYAN}{Colors.BOLD}â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—{Colors.RESET}")
        print(f"{Colors.BRIGHT_CYAN}{Colors.BOLD}â•‘  BYPASS MODE CONFIGURATION{Colors.RESET}")
        print(f"{Colors.BRIGHT_CYAN}{Colors.BOLD}â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•{Colors.RESET}\n")
        print(f"{Colors.BRIGHT_WHITE}Bypass/Auto-Yes Mode: {status}{Colors.RESET}\n")

        if self.bypass_mode:
            print(f"{Colors.BRIGHT_YELLOW}âš   WARNING: All confirmations will be automatically accepted!{Colors.RESET}")
            print(f"{Colors.YELLOW}   This mode will skip all permission prompts.{Colors.RESET}\n")
        else:
            print(f"{Colors.BRIGHT_GREEN}âœ“ Normal mode: Confirmations will be requested.{Colors.RESET}\n")

        input(f"{Colors.BRIGHT_CYAN}Press Enter to continue...{Colors.RESET}")

    def _bypass_indicator(self):
        """Show inline bypass mode indicator"""
        if self.bypass_mode:
            return f"{Colors.BG_YELLOW}{Colors.BLACK} AUTO-YES {Colors.RESET} "
        return ""

    def _confirm(self, prompt_message, default_yes=True):
        """Smart confirmation that respects bypass mode"""
        if self.bypass_mode:
            print(f"{self._bypass_indicator()}{Colors.DIM}{prompt_message} [Auto-accepted]{Colors.RESET}")
            return default_yes

        response = input(f"{prompt_message} ").strip().lower()
        return response in ['', 'y', 'yes'] if default_yes else response in ['y', 'yes']

    def print_banner(self):
        """Print colorful banner"""
        print(f"\n{Colors.BRIGHT_CYAN}{Colors.BOLD}")
        print("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
        print("â•‘                                                                                â•‘")
        print("â•‘                         ğŸ¤–  AI ROUTER CLI v1.0  ğŸ¤–                            â•‘")
        print("â•‘                                                                                â•‘")
        print("â•‘           Intelligent Model Selection & Execution Framework                   â•‘")
        print("â•‘              Based on 2025 Research (Sep-Nov 2025)                            â•‘")
        print("â•‘                                                                                â•‘")
        print("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
        print(Colors.RESET)

        # Bypass mode indicator
        if self.bypass_mode:
            print(f"\n{Colors.BG_YELLOW}{Colors.BLACK}{Colors.BOLD} âš  AUTO-YES MODE ACTIVE - ALL PROMPTS AUTO-ACCEPTED âš  {Colors.RESET}")
            print(f"{Colors.BRIGHT_YELLOW}Bypass mode is ENABLED. Use menu option [8] to disable.{Colors.RESET}\n")

        # Platform info
        if self.platform == "Darwin":
            platform_name = f"{Colors.BRIGHT_GREEN}MacBook M4 Pro (MLX Optimized){Colors.RESET}"
        else:
            platform_name = f"{Colors.BRIGHT_YELLOW}RTX 3090 (WSL Optimized){Colors.RESET}"

        print(f"\n{Colors.BRIGHT_WHITE}Platform: {platform_name}")
        print(f"{Colors.BRIGHT_WHITE}Available Models: {Colors.BRIGHT_CYAN}{len(self.models)}{Colors.RESET}")
        print()

    def print_model_info(self, model_id, model_data):
        """Display detailed model information"""
        print(f"\n{Colors.BRIGHT_CYAN}{Colors.BOLD}â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—{Colors.RESET}")
        print(f"{Colors.BRIGHT_CYAN}{Colors.BOLD}â•‘  MODEL INFORMATION{Colors.RESET}")
        print(f"{Colors.BRIGHT_CYAN}{Colors.BOLD}â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•{Colors.RESET}\n")

        print(f"{Colors.BRIGHT_WHITE}Name:          {Colors.BRIGHT_GREEN}{model_data['name']}{Colors.RESET}")
        print(f"{Colors.BRIGHT_WHITE}Model ID:      {Colors.BRIGHT_YELLOW}{model_id}{Colors.RESET}")
        print(f"{Colors.BRIGHT_WHITE}Size:          {Colors.BRIGHT_MAGENTA}{model_data['size']}{Colors.RESET}")
        print(f"{Colors.BRIGHT_WHITE}Speed:         {Colors.BRIGHT_CYAN}{model_data['speed']}{Colors.RESET}")
        print(f"{Colors.BRIGHT_WHITE}Framework:     {Colors.BRIGHT_BLUE}{model_data['framework'].upper()}{Colors.RESET}")
        print(f"{Colors.BRIGHT_WHITE}Use Case:      {Colors.YELLOW}{model_data['use_case']}{Colors.RESET}")
        print(f"{Colors.BRIGHT_WHITE}Context:       {Colors.CYAN}{model_data['context']:,} tokens{Colors.RESET}")

        print(f"\n{Colors.BRIGHT_WHITE}Optimal Parameters:{Colors.RESET}")
        print(f"  {Colors.WHITE}Temperature:   {Colors.GREEN}{model_data['temperature']}{Colors.RESET}")
        print(f"  {Colors.WHITE}Top-P:         {Colors.GREEN}{model_data['top_p']}{Colors.RESET}")
        print(f"  {Colors.WHITE}Top-K:         {Colors.GREEN}{model_data['top_k']}{Colors.RESET}")

        if model_data['special_flags']:
            print(f"\n{Colors.BRIGHT_YELLOW}Special Flags: {Colors.YELLOW}{' '.join(model_data['special_flags'])}{Colors.RESET}")

        if model_data['system_prompt']:
            print(f"\n{Colors.BRIGHT_GREEN}System Prompt: {Colors.GREEN}{model_data['system_prompt']}{Colors.RESET}")
        else:
            print(f"\n{Colors.BRIGHT_RED}âš   NO system prompt support{Colors.RESET}")

        if model_data['notes']:
            print(f"\n{Colors.BRIGHT_MAGENTA}Notes:{Colors.RESET}")
            print(f"  {Colors.MAGENTA}{model_data['notes']}{Colors.RESET}")

        print()

    def list_models(self):
        """List all available models with interactive selection"""
        print(f"\n{Colors.BRIGHT_CYAN}{Colors.BOLD}â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—{Colors.RESET}")
        print(f"{Colors.BRIGHT_CYAN}{Colors.BOLD}â•‘  AVAILABLE MODELS{Colors.RESET}")
        print(f"{Colors.BRIGHT_CYAN}{Colors.BOLD}â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•{Colors.RESET}\n")

        for idx, (model_id, model_data) in enumerate(self.models.items(), 1):
            print(f"{Colors.BRIGHT_YELLOW}[{idx}]{Colors.RESET} {Colors.BRIGHT_WHITE}{model_id}{Colors.RESET}")
            print(f"    {Colors.WHITE}{model_data['name']}{Colors.RESET}")
            print(f"    {Colors.CYAN}Use case: {model_data['use_case']}{Colors.RESET}")
            print(f"    {Colors.GREEN}{model_data['size']} | {model_data['speed']}{Colors.RESET}")
            print()

        # Interactive selection
        model_ids = list(self.models.keys())
        print(f"{Colors.BRIGHT_GREEN}[0]{Colors.RESET} {Colors.WHITE}Return to main menu{Colors.RESET}\n")
        choice = input(f"{Colors.BRIGHT_YELLOW}Select model number [0-{len(model_ids)}] or 'back': {Colors.RESET}").strip()

        if choice == "0" or choice.lower() == 'back':
            return

        try:
            idx = int(choice) - 1
            if 0 <= idx < len(model_ids):
                model_id = model_ids[idx]
                model_data = self.models[model_id]

                self.print_model_info(model_id, model_data)

                prompt = input(f"\n{Colors.BRIGHT_CYAN}Enter your prompt (or 'back' to return): {Colors.RESET}").strip()
                if prompt.lower() != 'back' and prompt:
                    self.run_model(model_id, model_data, prompt)
            else:
                print(f"{Colors.BRIGHT_RED}Invalid model number.{Colors.RESET}")
        except ValueError:
            print(f"{Colors.BRIGHT_RED}Invalid input.{Colors.RESET}")

    def interactive_mode(self):
        """Interactive model selection"""
        self.print_banner()

        while True:
            print(f"\n{Colors.BRIGHT_CYAN}{Colors.BOLD}â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•{Colors.RESET}")
            print(f"{Colors.BRIGHT_WHITE}What would you like to do?{Colors.RESET}\n")
            print(f"{Colors.BRIGHT_GREEN}[1]{Colors.RESET} ğŸ¯ Auto-select model based on prompt")
            print(f"{Colors.BRIGHT_GREEN}[2]{Colors.RESET} ğŸ“‹ Browse & select from all models")
            print(f"{Colors.BRIGHT_GREEN}[3]{Colors.RESET} ğŸ“ Context Management (Load files/text)")
            print(f"{Colors.BRIGHT_GREEN}[4]{Colors.RESET} ğŸ’¬ View system prompt examples")
            print(f"{Colors.BRIGHT_GREEN}[5]{Colors.RESET} âš™ï¸ View optimal parameters guide")
            print(f"{Colors.BRIGHT_GREEN}[6]{Colors.RESET} ğŸ“š View documentation guides")

            # Bypass mode toggle option
            bypass_status = f"{Colors.BRIGHT_GREEN}ON{Colors.RESET}" if self.bypass_mode else f"{Colors.BRIGHT_RED}OFF{Colors.RESET}"
            print(f"{Colors.BRIGHT_GREEN}[7]{Colors.RESET} ğŸ”“ Toggle Auto-Yes Mode (Currently: {bypass_status})")

            print(f"{Colors.BRIGHT_GREEN}[8]{Colors.RESET} ğŸšª Exit")
            print(f"{Colors.BRIGHT_CYAN}{Colors.BOLD}â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•{Colors.RESET}")

            # Show bypass indicator if active
            if self.bypass_mode:
                print(f"\n{self._bypass_indicator()}{Colors.YELLOW}Auto-Yes mode is active{Colors.RESET}")

            choice = input(f"\n{Colors.BRIGHT_YELLOW}Enter choice [1-8]: {Colors.RESET}").strip()

            if choice == "1":
                self.auto_select_mode()
            elif choice == "2":
                self.list_models()
            elif choice == "3":
                self.context_mode()
            elif choice == "4":
                self.view_system_prompts()
            elif choice == "5":
                self.view_parameters_guide()
            elif choice == "6":
                self.view_documentation()
            elif choice == "7":
                self.toggle_bypass_mode()
            elif choice == "8":
                print(f"\n{Colors.BRIGHT_GREEN}Goodbye!{Colors.RESET}\n")
                sys.exit(0)
            else:
                print(f"{Colors.BRIGHT_RED}Invalid choice. Please try again.{Colors.RESET}")

    def auto_select_mode(self):
        """Auto-select model based on prompt"""
        print(f"\n{Colors.BRIGHT_CYAN}Enter your prompt (or 'back' to return):{Colors.RESET}")
        prompt = input(f"{Colors.YELLOW}> {Colors.RESET}").strip()

        if prompt.lower() == 'back':
            return

        # Detect use case
        use_case = ModelDatabase.detect_use_case(prompt)
        print(f"\n{Colors.BRIGHT_MAGENTA}Detected use case: {Colors.BRIGHT_WHITE}{use_case.upper()}{Colors.RESET}")

        # Recommend model
        model_id, model_data = ModelDatabase.recommend_model(use_case, self.models)
        print(f"{Colors.BRIGHT_GREEN}Recommended model: {Colors.BRIGHT_WHITE}{model_data['name']}{Colors.RESET}")

        # Show model info
        self.print_model_info(model_id, model_data)

        # Ask if user wants to run (respects bypass mode)
        if self._confirm(f"\n{Colors.BRIGHT_YELLOW}Run this model? [Y/n]:{Colors.RESET}", default_yes=True):
            self.run_model(model_id, model_data, prompt)

    def manual_select_mode(self):
        """Manually select model"""
        self.list_models()

        model_ids = list(self.models.keys())
        choice = input(f"\n{Colors.BRIGHT_YELLOW}Select model number (or 'back'): {Colors.RESET}").strip()

        if choice.lower() == 'back':
            return

        try:
            idx = int(choice) - 1
            if 0 <= idx < len(model_ids):
                model_id = model_ids[idx]
                model_data = self.models[model_id]

                self.print_model_info(model_id, model_data)

                prompt = input(f"\n{Colors.BRIGHT_CYAN}Enter your prompt: {Colors.RESET}").strip()
                self.run_model(model_id, model_data, prompt)
            else:
                print(f"{Colors.BRIGHT_RED}Invalid model number.{Colors.RESET}")
        except ValueError:
            print(f"{Colors.BRIGHT_RED}Invalid input.{Colors.RESET}")

    def parse_llama_output(self, raw_output: str) -> tuple[str, int, int]:
        """Parse llama.cpp output to extract response text and token counts.

        Returns: (response_text, input_tokens, output_tokens)
        """
        # Initialize defaults
        response_text = ""
        input_tokens = 0
        output_tokens = 0

        # Split stderr and stdout if present
        lines = raw_output.split('\n')

        # Extract the actual response (after the prompt echo)
        # Look for the line after verbose prompt output
        in_response = False
        response_lines = []

        for line in lines:
            # Skip llama.cpp metadata and logging
            if any(keyword in line for keyword in ['llama_', 'ggml_', 'build info:', 'system info:', 'sampler']):
                continue

            # Look for token count info
            if 'prompt eval time' in line or 'eval time' in line:
                # Extract token counts from lines like:
                # "prompt eval time = 123.45 ms / 100 tokens"
                # "eval time = 234.56 ms / 50 tokens"
                token_match = re.search(r'/\s*(\d+)\s+tokens', line)
                if token_match:
                    tokens = int(token_match.group(1))
                    if 'prompt eval' in line:
                        input_tokens = tokens
                    else:
                        output_tokens = tokens
                continue

            # Skip empty lines at the start
            if not in_response and line.strip():
                in_response = True

            if in_response:
                response_lines.append(line)

        # Join response lines and clean up
        response_text = '\n'.join(response_lines).strip()

        # Remove any trailing metadata that might have slipped through
        if 'llama_perf_' in response_text:
            response_text = response_text.split('llama_perf_')[0].strip()

        return response_text, input_tokens, output_tokens

    def run_model(self, model_id, model_data, prompt):
        """Execute the model with optimal parameters and return ModelResponse"""
        print(f"\n{Colors.BRIGHT_GREEN}{Colors.BOLD}Launching {model_data['name']}...{Colors.RESET}\n")

        start_time = time.time()

        if model_data['framework'] == 'mlx':
            response = self.run_mlx_model(model_id, model_data, prompt)
        else:
            response = self.run_llamacpp_model(model_id, model_data, prompt)

        if response:
            response.duration_seconds = time.time() - start_time

        return response

    def run_llamacpp_model(self, model_data, prompt):
        """Run model using llama.cpp (WSL)"""
        # Load system prompt if available
        system_prompt = ""
        if model_data['system_prompt']:
            prompt_file = self.system_prompts_dir / model_data['system_prompt']
            if prompt_file.exists():
                try:
                    with open(prompt_file, 'r', encoding='utf-8') as f:
                        system_prompt = f.read().strip()
                except Exception as e:
                    print(f"{Colors.BRIGHT_YELLOW}Warning: Could not read system prompt file: {e}{Colors.RESET}")
                    print(f"{Colors.BRIGHT_YELLOW}Continuing without system prompt...{Colors.RESET}\n")

        # Build command with 2025 optimal parameters
        special_flags = ' '.join(model_data['special_flags'])

        cmd = f"""wsl bash -c "~/llama.cpp/build/bin/llama-cli \\
  -m '{model_data['path']}' \\
  -p '{prompt}' \\
  -ngl 999 \\
  -t 24 \\
  -b 512 \\
  -ub 512 \\
  -fa 1 \\
  --cache-type-k q8_0 \\
  --cache-type-v q8_0 \\
  --no-ppl \\
  --temp {model_data['temperature']} \\
  --top-p {model_data['top_p']} \\
  --top-k {model_data['top_k']} \\
  -c {model_data['context']} \\
  -ptc 10 \\
  --verbose-prompt \\
  --log-colors \\
  {special_flags} \\
  --mlock"
"""

        if system_prompt:
            cmd = cmd.replace(f"-p '{prompt}'", f"--system-prompt '{system_prompt}' -p '{prompt}'")

        print(f"{Colors.BRIGHT_CYAN}Executing command:{Colors.RESET}")
        print(f"{Colors.DIM}{cmd}{Colors.RESET}\n")

        # Execute
        result = subprocess.run(cmd, shell=True)
        if result.returncode != 0:
            print(f"\n{Colors.BRIGHT_RED}Error: Model execution failed with return code {result.returncode}{Colors.RESET}")
            print(f"{Colors.BRIGHT_YELLOW}Please check that llama.cpp is installed and the model path is correct.{Colors.RESET}\n")

    def run_mlx_model(self, model_data, prompt):
        """Run model using MLX (macOS)"""
        # Load system prompt if available
        system_prompt = ""
        if model_data['system_prompt']:
            prompt_file = self.system_prompts_dir / model_data['system_prompt']
            if prompt_file.exists():
                try:
                    with open(prompt_file, 'r', encoding='utf-8') as f:
                        system_prompt = f.read().strip()
                except Exception as e:
                    print(f"{Colors.BRIGHT_YELLOW}Warning: Could not read system prompt file: {e}{Colors.RESET}")
                    print(f"{Colors.BRIGHT_YELLOW}Continuing without system prompt...{Colors.RESET}\n")

        # Build MLX command
        cmd = f"""mlx_lm.generate \\
  --model {model_data['path']} \\
  --prompt "{prompt}" \\
  --max-tokens 2048 \\
  --temp {model_data['temperature']} \\
  --top-p {model_data['top_p']}"""

        if system_prompt:
            cmd = cmd.replace(f'--prompt "{prompt}"',
                            f'--system-prompt "{system_prompt}" --prompt "{prompt}"')

        print(f"{Colors.BRIGHT_CYAN}Executing command:{Colors.RESET}")
        print(f"{Colors.DIM}{cmd}{Colors.RESET}\n")

        # Execute
        result = subprocess.run(cmd, shell=True)
        if result.returncode != 0:
            print(f"\n{Colors.BRIGHT_RED}Error: Model execution failed with return code {result.returncode}{Colors.RESET}")
            print(f"{Colors.BRIGHT_YELLOW}Please check that MLX is installed and the model path is correct.{Colors.RESET}\n")

    def view_system_prompts(self):
        """Display system prompt examples"""
        print(f"\n{Colors.BRIGHT_CYAN}{Colors.BOLD}â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—{Colors.RESET}")
        print(f"{Colors.BRIGHT_CYAN}{Colors.BOLD}â•‘  SYSTEM PROMPT GUIDE{Colors.RESET}")
        print(f"{Colors.BRIGHT_CYAN}{Colors.BOLD}â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•{Colors.RESET}\n")

        print(f"{Colors.BRIGHT_WHITE}System prompts are stored in:{Colors.RESET}")
        print(f"{Colors.CYAN}{self.system_prompts_dir}{Colors.RESET}\n")

        print(f"{Colors.BRIGHT_WHITE}How to use system prompts:{Colors.RESET}\n")
        print(f"{Colors.WHITE}1. System prompts are automatically loaded by AI Router{Colors.RESET}")
        print(f"{Colors.WHITE}2. Each model has an optimized system prompt file{Colors.RESET}")
        print(f"{Colors.WHITE}3. You can edit these files to customize behavior{Colors.RESET}\n")

        print(f"{Colors.BRIGHT_YELLOW}Example system prompt structure:{Colors.RESET}\n")
        print(f"{Colors.GREEN}You are an expert AI assistant specialized in [domain].{Colors.RESET}")
        print(f"{Colors.GREEN}Your responses should be:{Colors.RESET}")
        print(f"{Colors.GREEN}- Accurate and well-researched{Colors.RESET}")
        print(f"{Colors.GREEN}- Clear and concise{Colors.RESET}")
        print(f"{Colors.GREEN}- Formatted with proper code blocks when relevant{Colors.RESET}\n")

        print(f"{Colors.BRIGHT_MAGENTA}Available system prompt files:{Colors.RESET}\n")
        for model_id, model_data in self.models.items():
            if model_data['system_prompt']:
                print(f"  {Colors.GREEN}âœ“{Colors.RESET} {model_data['system_prompt']}")
            else:
                print(f"  {Colors.RED}âœ—{Colors.RESET} {model_id} (no system prompt support)")
        print()

    def view_parameters_guide(self):
        """Display optimal parameters guide"""
        print(f"\n{Colors.BRIGHT_CYAN}{Colors.BOLD}â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—{Colors.RESET}")
        print(f"{Colors.BRIGHT_CYAN}{Colors.BOLD}â•‘  OPTIMAL PARAMETERS GUIDE (2025 Research){Colors.RESET}")
        print(f"{Colors.BRIGHT_CYAN}{Colors.BOLD}â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•{Colors.RESET}\n")

        print(f"{Colors.BRIGHT_WHITE}RTX 3090 (llama.cpp WSL):{Colors.RESET}\n")
        print(f"{Colors.GREEN}-ngl 999{Colors.RESET}           {Colors.WHITE}Full GPU offload (Aug 2025 default){Colors.RESET}")
        print(f"{Colors.GREEN}-t 24{Colors.RESET}             {Colors.WHITE}Use all CPU threads (Ryzen 9 3900X){Colors.RESET}")
        print(f"{Colors.GREEN}-b 512{Colors.RESET}            {Colors.WHITE}Minimum batch size (2025 research){Colors.RESET}")
        print(f"{Colors.GREEN}-ub 512{Colors.RESET}           {Colors.WHITE}Logical batch for prompt processing{Colors.RESET}")
        print(f"{Colors.GREEN}-fa 1{Colors.RESET}             {Colors.WHITE}Flash Attention (+20% speed, 50% memory){Colors.RESET}")
        print(f"{Colors.GREEN}--cache-type-k q8_0{Colors.RESET}  {Colors.WHITE}KV cache quantization (50% memory){Colors.RESET}")
        print(f"{Colors.GREEN}--cache-type-v q8_0{Colors.RESET}  {Colors.WHITE}KV cache quantization{Colors.RESET}")
        print(f"{Colors.GREEN}--no-ppl{Colors.RESET}          {Colors.WHITE}Skip perplexity (+15% speedup){Colors.RESET}")
        print(f"{Colors.GREEN}--mlock{Colors.RESET}           {Colors.WHITE}Lock model in RAM{Colors.RESET}\n")

        print(f"{Colors.BRIGHT_WHITE}MacBook M4 Pro (MLX):{Colors.RESET}\n")
        print(f"{Colors.GREEN}--max-tokens 2048{Colors.RESET}  {Colors.WHITE}Maximum generation length{Colors.RESET}")
        print(f"{Colors.GREEN}--temp 0.7{Colors.RESET}        {Colors.WHITE}Temperature (creativity){Colors.RESET}")
        print(f"{Colors.GREEN}--top-p 0.9{Colors.RESET}       {Colors.WHITE}Nucleus sampling{Colors.RESET}\n")

        print(f"{Colors.BRIGHT_WHITE}Temperature Guide:{Colors.RESET}\n")
        print(f"{Colors.CYAN}0.2-0.4{Colors.RESET}  {Colors.WHITE}Technical docs, code, Q&A (conservative){Colors.RESET}")
        print(f"{Colors.CYAN}0.5-0.9{Colors.RESET}  {Colors.WHITE}Content creation, conversation (balanced){Colors.RESET}")
        print(f"{Colors.CYAN}0.8-1.2+{Colors.RESET} {Colors.WHITE}Creative writing, brainstorming{Colors.RESET}\n")

        print(f"{Colors.BRIGHT_YELLOW}CRITICAL Notes:{Colors.RESET}\n")
        print(f"{Colors.RED}âœ—{Colors.RESET} {Colors.WHITE}NEVER use temp 0 with Qwen models (causes loops){Colors.RESET}")
        print(f"{Colors.RED}âœ—{Colors.RESET} {Colors.WHITE}NEVER use 'think step-by-step' with reasoning models{Colors.RESET}")
        print(f"{Colors.GREEN}âœ“{Colors.RESET} {Colors.WHITE}ALWAYS use --jinja flag with Phi-4{Colors.RESET}")
        print(f"{Colors.GREEN}âœ“{Colors.RESET} {Colors.WHITE}ALWAYS use --jinja flag with Qwen3{Colors.RESET}")
        print(f"{Colors.GREEN}âœ“{Colors.RESET} {Colors.WHITE}WSL provides near-native Linux performance (within 1%){Colors.RESET}\n")

    def view_documentation(self):
        """Display documentation guide menu"""
        docs_dir = self.models_dir

        # Define documentation files with priority (ONLY FILES THAT EXIST)
        docs = [
            # Frequently needed (top priority)
            ("HOW-TO-RUN-AI-ROUTER.md", "ğŸš€ How to Run the AI Router", "Getting started, usage, troubleshooting"),
            ("BOT-PROJECT-QUICK-START.md", "ğŸ¤– Bot & Project Management", "Create bots and projects with custom configs"),
            ("SYSTEM-PROMPTS-QUICK-START.md", "ğŸ“ System Prompts Quick Start", "Using and customizing system prompts"),

            # Important reference
            ("COMPREHENSIVE-EVALUATION-FRAMEWORK-PROMPT.md", "ğŸ“Š Evaluation Framework", "Testing and comparing models"),
            ("2025-RESEARCH-SUMMARY.md", "ğŸ”¬ 2025 Research Summary", "Latest research findings and best practices"),
            ("MACBOOK-M4-OPTIMIZATION-GUIDE.md", "ğŸ’» MacBook M4 Optimization", "Optimizing for Apple M4 hardware"),

            # Additional resources
            ("GITHUB-SETUP-GUIDE.md", "ğŸ™ GitHub Setup Guide", "Setting up Git and GitHub"),
            ("README.md", "ğŸ“– Project README", "Complete project overview and setup"),
        ]

        # Filter to only files that actually exist
        existing_docs = []
        for filename, title, desc in docs:
            doc_path = docs_dir / filename
            if doc_path.exists():
                existing_docs.append((filename, title, desc))

        docs = existing_docs

        if not docs:
            print(f"\n{Colors.BRIGHT_RED}No documentation files found!{Colors.RESET}\n")
            input(f"{Colors.BRIGHT_YELLOW}Press Enter to return...{Colors.RESET}")
            return

        print(f"\n{Colors.BRIGHT_CYAN}{Colors.BOLD}â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—{Colors.RESET}")
        print(f"{Colors.BRIGHT_CYAN}{Colors.BOLD}â•‘  ğŸ“š DOCUMENTATION GUIDES ({len(docs)} available){Colors.RESET}")
        print(f"{Colors.BRIGHT_CYAN}{Colors.BOLD}â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•{Colors.RESET}\n")

        # Display docs organized by priority
        # Top 3 are most frequently needed
        if len(docs) >= 1:
            print(f"{Colors.BRIGHT_WHITE}â­ Most Frequently Needed:{Colors.RESET}\n")
            for idx in range(min(3, len(docs))):
                filename, title, desc = docs[idx]
                print(f"{Colors.BRIGHT_GREEN}[{idx+1}]{Colors.RESET} {Colors.BRIGHT_WHITE}{title}{Colors.RESET}")
                print(f"    {Colors.CYAN}{desc}{Colors.RESET}")
                print()

        # Next 3 are important reference
        if len(docs) > 3:
            print(f"{Colors.BRIGHT_WHITE}ğŸ“š Important Reference:{Colors.RESET}\n")
            for idx in range(3, min(6, len(docs))):
                filename, title, desc = docs[idx]
                print(f"{Colors.BRIGHT_GREEN}[{idx+1}]{Colors.RESET} {Colors.WHITE}{title}{Colors.RESET}")
                print(f"    {Colors.CYAN}{desc}{Colors.RESET}")
                print()

        # Remaining are additional resources
        if len(docs) > 6:
            print(f"{Colors.BRIGHT_WHITE}ğŸ“– Additional Resources:{Colors.RESET}\n")
            for idx in range(6, len(docs)):
                filename, title, desc = docs[idx]
                print(f"{Colors.BRIGHT_GREEN}[{idx+1}]{Colors.RESET} {Colors.WHITE}{title}{Colors.RESET}")
                print(f"    {Colors.CYAN}{desc}{Colors.RESET}")
                print()

        print(f"{Colors.BRIGHT_GREEN}[0]{Colors.RESET} {Colors.WHITE}Return to main menu{Colors.RESET}\n")

        choice = input(f"{Colors.BRIGHT_YELLOW}Select documentation [0-{len(docs)}]: {Colors.RESET}").strip()

        if choice == "0":
            return

        try:
            idx = int(choice) - 1
            if 0 <= idx < len(docs):
                filename, title, desc = docs[idx]
                doc_path = docs_dir / filename

                if doc_path.exists():
                    # Read and display the documentation
                    with open(doc_path, 'r', encoding='utf-8') as f:
                        content = f.read()

                    print(f"\n{Colors.BRIGHT_CYAN}{Colors.BOLD}â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—{Colors.RESET}")
                    print(f"{Colors.BRIGHT_CYAN}{Colors.BOLD}â•‘  {title}{Colors.RESET}")
                    print(f"{Colors.BRIGHT_CYAN}{Colors.BOLD}â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•{Colors.RESET}\n")

                    # Display content with pagination (first 50 lines)
                    lines = content.split('\n')
                    page_size = 50

                    for i in range(0, len(lines), page_size):
                        page_lines = lines[i:i+page_size]
                        print('\n'.join(page_lines))

                        if i + page_size < len(lines):
                            cont = input(f"\n{Colors.BRIGHT_YELLOW}Press Enter to continue, 'q' to quit: {Colors.RESET}").strip().lower()
                            if cont == 'q':
                                break
                        else:
                            print(f"\n{Colors.BRIGHT_GREEN}[End of document]{Colors.RESET}")

                    input(f"\n{Colors.BRIGHT_YELLOW}Press Enter to return to menu...{Colors.RESET}")
                else:
                    print(f"{Colors.BRIGHT_RED}Documentation file not found: {filename}{Colors.RESET}")
                    input(f"\n{Colors.BRIGHT_YELLOW}Press Enter to continue...{Colors.RESET}")
            else:
                print(f"{Colors.BRIGHT_RED}Invalid selection.{Colors.RESET}")
        except ValueError:
            print(f"{Colors.BRIGHT_RED}Invalid input.{Colors.RESET}")


def main():
    """Main entry point"""
    try:
        router = AIRouter()

        # Check if arguments provided (non-interactive mode)
        if len(sys.argv) > 1:
            if sys.argv[1] == "--list":
                router.print_banner()
                router.list_models()
            elif sys.argv[1] == "--help":
                router.print_banner()
                print(f"{Colors.BRIGHT_WHITE}Usage:{Colors.RESET}\n")
                print(f"  {Colors.GREEN}python ai-router.py{Colors.RESET}")
                print(f"    {Colors.WHITE}Launch interactive mode{Colors.RESET}\n")
                print(f"  {Colors.GREEN}python ai-router.py --list{Colors.RESET}")
                print(f"    {Colors.WHITE}List all available models{Colors.RESET}\n")
                print(f"  {Colors.GREEN}python ai-router.py --help{Colors.RESET}")
                print(f"    {Colors.WHITE}Show this help message{Colors.RESET}\n")
            else:
                print(f"{Colors.BRIGHT_RED}Unknown argument. Use --help for usage.{Colors.RESET}")
        else:
            # Interactive mode
            router.interactive_mode()

    except KeyboardInterrupt:
        print(f"\n\n{Colors.BRIGHT_YELLOW}Interrupted by user. Goodbye!{Colors.RESET}\n")
        sys.exit(0)
    except Exception as e:
        print(f"\n{Colors.BRIGHT_RED}Error: {e}{Colors.RESET}\n")
        sys.exit(1)


if __name__ == "__main__":
    main()
