â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                                â•‘
â•‘                    MODEL DELETION PLAN - REMOVE DUPLICATES                     â•‘
â•‘                   After Upgrades Complete (2025-12-08)                         â•‘
â•‘                                                                                â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
OVERVIEW: DOWNLOADS IN PROGRESS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Currently downloading (6 models in parallel):

RTX 3090 Models:
  â³ Qwen3-Coder-30B Q4_K_M (19GB)              [Replaces: Qwen2.5-Coder-32B]
  â³ Phi-4-reasoning-plus Q6_K (12GB)           [Replaces: Phi-4]
  â³ Gemma 3 27B Abliterated IQ2_M (12GB)       [Replaces: MythoMax-L2-13B]
  â³ Ministral-3-14B-Reasoning Q5_K_M (9.8GB)   [NEW - no replacement]

RTX 4060 Ti Models:
  â³ Qwen3-14B-Instruct Q4_K_M (8.4GB)          [Replaces: Qwen2.5-14B-Instruct]
  â³ Qwen3-8B-Instruct Q5_K_M (5.5GB)           [Replaces: Llama3.1-8B-Instruct]

Total New Storage: +47.7GB (temporary until old models deleted)
Net Storage After Deletions: +2.6GB


â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
SECTION 1: MODELS TO DELETE (5 MODELS, 56.5GB RECOVERED)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ğŸ—‘ï¸ DELETE 1: Qwen2.5-Coder-32B-Instruct-Q4_K_M.gguf (19GB)                 â”‚
â”‚ Location: D:\models\organized\                                             â”‚
â”‚ Status: CORRUPTED (confirmed null bytes)                                   â”‚
â”‚ Replaced By: Qwen3-Coder-30B Q4_K_M (19GB)                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Why Delete:
  âœ“ File is corrupted (null bytes instead of GGUF data)
  âœ“ Qwen3-Coder-30B is superior in every way:
    - 8x larger context (256K vs 32K)
    - Better benchmarks (94% vs 92.2% HumanEval)
    - Agentic workflow design
    - Released 2 days ago (Dec 2025 vs Oct 2024)
  âœ“ Same file size (19GB) - direct replacement

Delete Command:
  wsl bash -c "rm /mnt/d/models/organized/Qwen2.5-Coder-32B-Instruct-Q4_K_M.gguf"

Space Recovered: 19GB


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ğŸ—‘ï¸ DELETE 2: phi-4-Q6_K.gguf (12GB)                                        â”‚
â”‚ Location: D:\models\organized\                                             â”‚
â”‚ Status: Working, but vastly inferior to reasoning-plus variant             â”‚
â”‚ Replaced By: Phi-4-reasoning-plus Q6_K (12GB)                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Why Delete:
  âœ“ Phi-4-reasoning-plus has 189% better AIME score (78% vs 27%)
  âœ“ Same architecture, same file size (12GB Q6_K)
  âœ“ Reasoning-plus is specialized training on top of base Phi-4
  âœ“ No reason to keep inferior version
  âœ“ General capabilities maintained (86.4% MMLU same on both)

Performance Comparison:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Benchmark           â”‚ Base     â”‚ Reasoning-Plus     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ AIME 2024           â”‚ 27%      â”‚ 78% (+189%)        â”‚
â”‚ MATH-500            â”‚ 81%      â”‚ 90% (+11%)         â”‚
â”‚ MMLU                â”‚ 86.4%    â”‚ 86.4% (same)       â”‚
â”‚ ARC-Challenge       â”‚ 96.3%    â”‚ 96.3% (same)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Delete Command:
  wsl bash -c "rm /mnt/d/models/organized/phi-4-Q6_K.gguf"

Space Recovered: 12GB


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ğŸ—‘ï¸ DELETE 3: mythomax-l2-13b.Q6_K.gguf (10GB)                              â”‚
â”‚ Location: D:\models\organized\                                             â”‚
â”‚ Status: Working, but severely outdated for creative writing                â”‚
â”‚ Replaced By: Gemma 3 27B Abliterated IQ2_M (12GB)                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Why Delete:
  âœ“ Gemma 3 27B has 2x more parameters (27B vs 13B)
  âœ“ Context window: 128K vs 8K (16x improvement!)
  âœ“ Training data: 2024-2025 vs 2023 (much more current)
  âœ“ Google's latest Gemma 3 architecture (state-of-art)
  âœ“ Abliterated variant maintains uncensored capabilities
  âœ“ Better character consistency across long narratives

Feature Comparison:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Feature                 â”‚ MythoMax    â”‚ Gemma 3 27B     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Parameters              â”‚ 13B         â”‚ 27B (2x!)       â”‚
â”‚ Context Window          â”‚ 8K          â”‚ 128K (16x!)     â”‚
â”‚ Creative Score          â”‚ Good        â”‚ Excellent       â”‚
â”‚ Training Data           â”‚ 2023        â”‚ 2024-2025       â”‚
â”‚ Long Narrative Support  â”‚ 8K limit    â”‚ Novel-length    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Impact: Can now write 40-page stories vs 2-3 pages with MythoMax

Delete Command:
  wsl bash -c "rm /mnt/d/models/organized/mythomax-l2-13b.Q6_K.gguf"

Space Recovered: 10GB


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ğŸ—‘ï¸ DELETE 4: Qwen2.5-14B-Instruct Q4_K_M (8.4GB)                           â”‚
â”‚ Location: D:\models\rtx4060ti-16gb\qwen25-14b-instruct\                   â”‚
â”‚ Status: Working, but completely superseded by Qwen3                        â”‚
â”‚ Replaced By: Qwen3-14B-Instruct Q4_K_M (8.4GB)                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Why Delete:
  âœ“ Qwen3-14B matches Qwen2.5-32B performance (18B larger model!)
  âœ“ 4x larger context (128K vs 32K)
  âœ“ Better benchmarks across the board (82% vs 79% MMLU)
  âœ“ Released December 2025 (latest generation)
  âœ“ Same file size (8.4GB Q4_K_M) - direct replacement
  âœ“ Improved multilingual capabilities

Benchmark Comparison:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Benchmark               â”‚ Qwen2.5     â”‚ Qwen3           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ MMLU                    â”‚ 79%         â”‚ 82%             â”‚
â”‚ Context Window          â”‚ 32K         â”‚ 128K (4x!)      â”‚
â”‚ HumanEval               â”‚ 70%         â”‚ 75%             â”‚
â”‚ Performance Equivalent  â”‚ Self        â”‚ Qwen2.5-32B!    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Delete Command:
  wsl bash -c "rm -rf /mnt/d/models/rtx4060ti-16gb/qwen25-14b-instruct"

Space Recovered: 8.4GB


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ğŸ—‘ï¸ DELETE 5: Llama3.1-8B-Instruct Q6_K (6.2GB)                             â”‚
â”‚ Location: D:\models\rtx4060ti-16gb\llama31-8b-instruct\                   â”‚
â”‚ Status: Working, but inferior to Qwen3-8B in every metric                 â”‚
â”‚ Replaced By: Qwen3-8B-Instruct Q5_K_M (5.5GB)                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Why Delete:
  âœ“ Qwen3-8B matches Qwen2.5-14B performance (6B larger!)
  âœ“ 4x larger context (128K vs 32K)
  âœ“ Better benchmarks (73% vs 68% MMLU)
  âœ“ Superior coding capabilities (68% vs 62% HumanEval)
  âœ“ Faster inference due to better optimization
  âœ“ Smaller file size (5.5GB vs 6.2GB) - saves space!

Performance Comparison:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Metric                  â”‚ Llama3.1    â”‚ Qwen3           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ MMLU                    â”‚ 68%         â”‚ 73%             â”‚
â”‚ Context Window          â”‚ 32K         â”‚ 128K (4x!)      â”‚
â”‚ HumanEval               â”‚ 62%         â”‚ 68%             â”‚
â”‚ Performance Equivalent  â”‚ Llama3.1-8B â”‚ Qwen2.5-14B!    â”‚
â”‚ File Size               â”‚ 6.2GB       â”‚ 5.5GB (saves!)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Delete Command:
  wsl bash -c "rm -rf /mnt/d/models/rtx4060ti-16gb/llama31-8b-instruct"

Space Recovered: 6.2GB + directory


â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
SECTION 2: MODELS TO KEEP (NO DUPLICATES, UNIQUE PURPOSES)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

These models serve unique purposes and have NO superior replacements:


âœ… KEEP: Llama 3.3 70B Abliterated IQ2_S (21GB)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   Why: Best uncensored 70B model available
   Unique: Largest parameter count in arsenal (70B)
   Use: Most capable model for complex unrestricted research
   Alternative: None at this parameter size that fits in 24GB

âœ… KEEP: Dolphin-Mistral-24B-Venice Q4_K_M (14GB)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   Why: Lowest refusal rate (2.2%) among all models
   Unique: Specialized for compliance with sensitive requests
   Use: When Llama 3.3 70B refuses or you need faster 24B model
   Alternative: None with this low refusal rate

âœ… KEEP: DeepSeek-R1-Distill-Qwen-14B Q5_K_M (9.8GB)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   Why: Specialized chain-of-thought reasoning
   Unique: Explicit reasoning step display (different from Phi-4/Ministral)
   Use: When you need transparent reasoning process shown
   Alternative: Ministral-3-14B (different approach, complementary)
   Note: Could be upgraded to DeepSeek-R1-32B Abliterated (optional)

âœ… KEEP: Dolphin3.0-Llama3.1-8B Q6_K (6.2GB) [BOTH GPUS]
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   Why: Fastest uncensored model (60-90 tok/sec)
   Unique: 8B uncensored with excellent speed
   Use: Quick uncensored responses, testing, rapid prototyping
   Alternative: Qwen3-8B is faster but CENSORED
   Keep on: Both RTX 3090 and RTX 4060 Ti

âœ… KEEP: Wizard-Vicuna-13B-Uncensored Q4_0 (6.9GB)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   Why: Classic uncensored variant, proven reliability
   Unique: Different architecture from Llama/Qwen/Dolphin family
   Use: When other uncensored models behave unexpectedly
   Alternative: Dolphin 8B (but different capabilities)

âœ… KEEP: Qwen 2.5 14B Uncensored Q4_K_M (8.4GB) [RTX 4060 Ti]
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   Why: Uncensored daily driver for 16GB GPU
   Unique: No Qwen3 uncensored version available yet
   Use: Best uncensored model for RTX 4060 Ti server
   Alternative: None (Qwen3-14B is censored)
   Note: KEEP until Qwen3-14B-Uncensored is released

âœ… KEEP: Qwen 2.5 Coder 7B Q5_K_M (5.1GB) [RTX 4060 Ti]
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   Why: Fast coding for RTX 4060 Ti (40-60 tok/sec, 84.8% HumanEval)
   Unique: Smallest competent coding model
   Use: Rapid coding tasks on 16GB GPU
   Alternative: Qwen3-Coder-30B (but that's on RTX 3090, needs 19GB)
   Note: Could upgrade to Qwen3-Coder-7B when available


â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
SECTION 3: AFTER UPGRADES - FINAL ARSENAL
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

RTX 3090 Models (D:\models\organized\) - 24GB VRAM:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
âœ… Llama 3.3 70B Abliterated IQ2_S (21GB)           [Best uncensored 70B]
âœ… Qwen3-Coder-30B Q4_K_M (19GB)                    [NEW - Best coding 2025]
âœ… Dolphin-Mistral-24B-Venice Q4_K_M (14GB)         [Lowest refusal 2.2%]
âœ… Gemma 3 27B Abliterated IQ2_M (12GB)             [NEW - Creative writing]
âœ… Phi-4-reasoning-plus Q6_K (12GB)                 [NEW - 78% AIME reasoning]
âœ… Ministral-3-14B-Reasoning Q5_K_M (9.8GB)         [NEW - 85% AIME reasoning]
âœ… DeepSeek-R1-Distill-Qwen-14B Q5_K_M (9.8GB)      [Chain-of-thought]
âœ… Wizard-Vicuna-13B-Uncensored Q4_0 (6.9GB)        [Classic uncensored]
âœ… Dolphin3.0-Llama3.1-8B Q6_K (6.2GB)              [Fast uncensored]

Total: 9 models, ~111GB


RTX 4060 Ti Models (D:\models\rtx4060ti-16gb\) - 16GB VRAM:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
âœ… Qwen3-14B-Instruct Q4_K_M (8.4GB)                [NEW - Daily driver]
âœ… Qwen 2.5 14B Uncensored Q4_K_M (8.4GB)           [Uncensored daily driver]
âœ… Dolphin3.0-Llama3.1-8B Q6_K (6.2GB)              [Fast uncensored]
âœ… Qwen3-8B-Instruct Q5_K_M (5.5GB)                 [NEW - Fast general purpose]
âœ… Qwen 2.5 Coder 7B Q5_K_M (5.1GB)                 [Fast coding]

Total: 5 models, ~33.6GB


COMBINED TOTAL: 14 models, ~144.6GB (down from 155.5GB pre-cleanup)


â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
SECTION 4: DELETION PROCEDURE
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

IMPORTANT: Wait for all downloads to complete before deleting old models!

Step 1: Verify All New Models Downloaded
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
wsl bash -c "ls -lh /mnt/d/models/organized/Qwen3-Coder*"
wsl bash -c "ls -lh /mnt/d/models/organized/Phi-4-reasoning-plus*"
wsl bash -c "ls -lh /mnt/d/models/organized/Gemma-3-27B*"
wsl bash -c "ls -lh /mnt/d/models/organized/Ministral-3-14B*"
wsl bash -c "ls -lh /mnt/d/models/rtx4060ti-16gb/qwen3-14b-instruct/*.gguf"
wsl bash -c "ls -lh /mnt/d/models/rtx4060ti-16gb/qwen3-8b-instruct/*.gguf"


Step 2: Test Each New Model (Verify GGUF Header)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Each should show: 0000000   G   G   U   F
wsl bash -c "head -c 4 /mnt/d/models/organized/Qwen3-Coder*.gguf | od -c"
wsl bash -c "head -c 4 /mnt/d/models/organized/Phi-4-reasoning-plus*.gguf | od -c"
wsl bash -c "head -c 4 /mnt/d/models/organized/Gemma-3-27B*.gguf | od -c"
wsl bash -c "head -c 4 /mnt/d/models/organized/Ministral-3-14B*.gguf | od -c"


Step 3: Test Load Each New Model
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
.\run-in-wsl.ps1 -ModelName "Qwen3 Coder" -Prompt "print hello world"
.\run-in-wsl.ps1 -ModelName "Phi-4-reasoning-plus" -Prompt "What is 2+2?"
.\run-in-wsl.ps1 -ModelName "Gemma 3" -Prompt "Write a short story"
.\run-in-wsl.ps1 -ModelName "Ministral" -Prompt "Solve: x^2 + 5x + 6 = 0"


Step 4: BATCH DELETE OLD MODELS (After Testing)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

COPY-PASTE THIS SCRIPT TO DELETE ALL 5 OLD MODELS AT ONCE:

wsl bash -c "
echo 'â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•'
echo 'Deleting old models to recover 55.8GB...'
echo 'â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•'
echo ''

echo 'ğŸ—‘ï¸  Deleting Qwen2.5-Coder-32B (19GB - corrupted)...'
rm -v /mnt/d/models/organized/Qwen2.5-Coder-32B-Instruct-Q4_K_M.gguf

echo 'ğŸ—‘ï¸  Deleting Phi-4 base (12GB - inferior to reasoning-plus)...'
rm -v /mnt/d/models/organized/phi-4-Q6_K.gguf

echo 'ğŸ—‘ï¸  Deleting MythoMax-L2-13B (10GB - outdated)...'
rm -v /mnt/d/models/organized/mythomax-l2-13b.Q6_K.gguf

echo 'ğŸ—‘ï¸  Deleting Qwen2.5-14B-Instruct (8.4GB - superseded by Qwen3)...'
rm -rfv /mnt/d/models/rtx4060ti-16gb/qwen25-14b-instruct

echo 'ğŸ—‘ï¸  Deleting Llama3.1-8B-Instruct (6.2GB - inferior to Qwen3-8B)...'
rm -rfv /mnt/d/models/rtx4060ti-16gb/llama31-8b-instruct

echo ''
echo 'âœ… Deletion complete! Checking remaining space...'
echo ''
du -sh /mnt/d/models/organized/
du -sh /mnt/d/models/rtx4060ti-16gb/
"


Step 5: INDIVIDUAL DELETE COMMANDS (If You Prefer Manual Control)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# Delete corrupted Qwen2.5-Coder
wsl bash -c "rm /mnt/d/models/organized/Qwen2.5-Coder-32B-Instruct-Q4_K_M.gguf"

# Delete base Phi-4
wsl bash -c "rm /mnt/d/models/organized/phi-4-Q6_K.gguf"

# Delete MythoMax
wsl bash -c "rm /mnt/d/models/organized/mythomax-l2-13b.Q6_K.gguf"

# Delete Qwen2.5-14B-Instruct
wsl bash -c "rm -rf /mnt/d/models/rtx4060ti-16gb/qwen25-14b-instruct"

# Delete Llama3.1-8B-Instruct
wsl bash -c "rm -rf /mnt/d/models/rtx4060ti-16gb/llama31-8b-instruct"


â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
SECTION 5: STORAGE IMPACT SUMMARY
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Current Storage (Before Cleanup):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
RTX 3090 (organized/):          99GB (9 models)
RTX 4060 Ti (rtx4060ti-16gb/):  40GB (5 models)
TOTAL:                          139GB

After New Downloads (Before Deletion):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
RTX 3090:                       148.8GB (temporary overlap)
RTX 4060 Ti:                    54GB (temporary overlap)
TOTAL:                          202.8GB (temporary peak)

After Cleanup (Final State):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
RTX 3090:                       111GB (9 models, all upgraded)
RTX 4060 Ti:                    33.6GB (5 models, all upgraded)
TOTAL:                          144.6GB

Net Change: +5.6GB (gained 1 new model: Ministral-3-14B-Reasoning)
Space Recovered from Deletions: 55.8GB


â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
SECTION 6: OPTIONAL - DEEP ANALYSIS ON "MAYBE DELETE" MODELS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

These models are in the "KEEP" list above, but could be reconsidered:


ğŸ¤” MAYBE DELETE: DeepSeek-R1-Distill-Qwen-14B Q5_K_M (9.8GB)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   Overlap With: Ministral-3-14B-Reasoning (85% AIME) & Phi-4-reasoning-plus (78% AIME)

   Pros of Keeping:
   âœ“ Different reasoning approach (explicit chain-of-thought display)
   âœ“ 94.3% MATH-500 (better than Ministral/Phi-4)
   âœ“ Only 9.8GB (same size as Ministral)

   Cons of Keeping:
   âœ— Overlaps with two other reasoning models
   âœ— Lower AIME score (not specified, but likely <85%)
   âœ— 32K context vs Ministral's 256K

   Recommendation: KEEP for now, but could delete if space is critical
   Savings if deleted: 9.8GB


ğŸ¤” MAYBE DELETE: Wizard-Vicuna-13B-Uncensored Q4_0 (6.9GB)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   Overlap With: Dolphin 8B (6.2GB, also uncensored, faster)

   Pros of Keeping:
   âœ“ Different architecture (may handle some prompts differently)
   âœ“ 13B parameters vs Dolphin's 8B (theoretically more capable)
   âœ“ Classic proven model

   Cons of Keeping:
   âœ— Older model (2023 training)
   âœ— Slower than Dolphin 8B (50-70 vs 60-90 tok/sec)
   âœ— No specific advantage over Dolphin 8B in practice
   âœ— Overlaps with multiple uncensored options

   Recommendation: KEEP for diversity, DELETE if space critical
   Savings if deleted: 6.9GB


If You Delete Both Optional Models: +16.7GB recovered
Final Total: 127.9GB (vs 144.6GB)


â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
SECTION 7: RECOMMENDED FINAL ACTION
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

âœ… RECOMMENDED PATH: Delete All 5 Primary Models
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   Models: Qwen2.5-Coder-32B, Phi-4, MythoMax, Qwen2.5-14B, Llama3.1-8B
   Space Recovered: 55.8GB
   Final Size: 144.6GB
   Result: Optimal balance of capability and storage

   This gives you:
   âœ“ All latest 2025 models
   âœ“ 8-16x larger context windows
   âœ“ Best benchmarks in every category
   âœ“ No functional duplicates
   âœ“ Maintained diversity (different architectures)


ğŸŸ¡ AGGRESSIVE PATH: Delete 7 Models (Include DeepSeek-R1 & Wizard-Vicuna)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   Models: Above 5 + DeepSeek-R1-14B + Wizard-Vicuna-13B
   Space Recovered: 72.5GB
   Final Size: 127.9GB
   Result: Maximum efficiency, slight redundancy reduction

   Trade-offs:
   âœ— Lose explicit chain-of-thought display (DeepSeek-R1)
   âœ— Lose architectural diversity (Wizard-Vicuna)
   âœ“ Still have Ministral-3 + Phi-4-plus for reasoning
   âœ“ Still have Dolphin 8B + others for uncensored


â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
END OF DELETION PLAN
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

NEXT STEPS:
1. Wait for all downloads to complete (check with ls commands in Step 1)
2. Verify new models work (Steps 2-3)
3. Run batch deletion script (Step 4)
4. Update model-registry.json with new models
5. Enjoy your state-of-the-art 2025 model arsenal!
